{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cbc54d5b-678c-4109-a587-6f019b928ddc",
   "metadata": {},
   "source": [
    "### 1. Backbone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "80d41a30-bb2d-4c9b-9377-7ab6a9cdb72f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mmdet.models import build_backbone\n",
    "from mmcv import Config\n",
    "\n",
    "cfg = Config.fromfile('configs/model_modules.py')\n",
    "backbone = build_backbone(cfg.backbone)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "7e5e8f8e-2211-4657-9ab0-319c77c46c76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input type: <class 'torch.Tensor'>\n",
      "input shape torch.Size([4, 3, 512, 512])\n",
      "\n",
      "output type: <class 'tuple'>\n",
      "output length: 4\n",
      "output item type: <class 'torch.Tensor'>\n",
      "output item shapes ((4, 256, 128, 128), (4, 512, 64, 64), (4, 1024, 32, 32), (4, 2048, 16, 16))\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "input = torch.randn(4, 3, 512, 512)\n",
    "output_backbone = backbone(input)\n",
    "\n",
    "print('input type:', type(input)) # = Tensor\n",
    "print('input shape', input.shape) # shape of feature maps from each stage\n",
    "print()\n",
    "print('output type:', type(output_backbone)) # = tuple\n",
    "print('output length:', len(output_backbone)) # = number of stages in backbone\n",
    "print('output item type:', type(output_backbone[0])) # = Tensor\n",
    "print('output item shapes', tuple(tuple(t.shape) for t in output_backbone)) # shape of feature maps from each stage"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8488aa4e-91ca-4f56-a994-eee733cbd2b7",
   "metadata": {},
   "source": [
    "### 2. Neck"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "b497736f-b87c-4cb4-9063-dcaf7eca2d6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output type: <class 'tuple'>\n",
      "output length: 5\n",
      "output item type: <class 'torch.Tensor'>\n",
      "output item shapes ((4, 256, 128, 128), (4, 256, 64, 64), (4, 256, 32, 32), (4, 256, 16, 16), (4, 256, 8, 8))\n"
     ]
    }
   ],
   "source": [
    "from mmdet.models import build_neck\n",
    "\n",
    "neck = build_neck(cfg.neck)\n",
    "output_neck = neck(output_backbone)\n",
    "\n",
    "print('output type:', type(output_neck)) # = tuple\n",
    "print('output length:', len(output_neck)) # = number of stages in backbone\n",
    "print('output item type:', type(output_neck[0])) # = Tensor\n",
    "print('output item shapes', tuple(tuple(t.shape) for t in output_neck)) # shape of feature maps from each stage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "9f7ed381-38c2-40fd-8966-f581ea3405f3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(128, 128), (64, 64), (32, 32), (16, 16), (8, 8)]"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[(512//n, 512//n) for n in [4, 8, 16, 32, 64]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "17629bcc-f417-4e62-bdb5-a2996c95d35e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mmdet.core import AnchorGenerator\n",
    "anchor_gen = AnchorGenerator(scales=[8],\n",
    "            ratios=[1.0],\n",
    "            strides=[4, 8, 16, 32, 64])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "999be144-807b-46e5-bca9-bf9724bb6606",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([[-16., -16.,  16.,  16.],\n",
       "         [-12., -16.,  20.,  16.],\n",
       "         [ -8., -16.,  24.,  16.],\n",
       "         ...,\n",
       "         [484., 492., 516., 524.],\n",
       "         [488., 492., 520., 524.],\n",
       "         [492., 492., 524., 524.]]),\n",
       " tensor([[-32., -32.,  32.,  32.],\n",
       "         [-24., -32.,  40.,  32.],\n",
       "         [-16., -32.,  48.,  32.],\n",
       "         ...,\n",
       "         [456., 472., 520., 536.],\n",
       "         [464., 472., 528., 536.],\n",
       "         [472., 472., 536., 536.]]),\n",
       " tensor([[-64., -64.,  64.,  64.],\n",
       "         [-48., -64.,  80.,  64.],\n",
       "         [-32., -64.,  96.,  64.],\n",
       "         ...,\n",
       "         [400., 432., 528., 560.],\n",
       "         [416., 432., 544., 560.],\n",
       "         [432., 432., 560., 560.]]),\n",
       " tensor([[-128., -128.,  128.,  128.],\n",
       "         [ -96., -128.,  160.,  128.],\n",
       "         [ -64., -128.,  192.,  128.],\n",
       "         ...,\n",
       "         [ 288.,  352.,  544.,  608.],\n",
       "         [ 320.,  352.,  576.,  608.],\n",
       "         [ 352.,  352.,  608.,  608.]]),\n",
       " tensor([[-256., -256.,  256.,  256.],\n",
       "         [-192., -256.,  320.,  256.],\n",
       "         [-128., -256.,  384.,  256.],\n",
       "         [ -64., -256.,  448.,  256.],\n",
       "         [   0., -256.,  512.,  256.],\n",
       "         [  64., -256.,  576.,  256.],\n",
       "         [ 128., -256.,  640.,  256.],\n",
       "         [ 192., -256.,  704.,  256.],\n",
       "         [-256., -192.,  256.,  320.],\n",
       "         [-192., -192.,  320.,  320.],\n",
       "         [-128., -192.,  384.,  320.],\n",
       "         [ -64., -192.,  448.,  320.],\n",
       "         [   0., -192.,  512.,  320.],\n",
       "         [  64., -192.,  576.,  320.],\n",
       "         [ 128., -192.,  640.,  320.],\n",
       "         [ 192., -192.,  704.,  320.],\n",
       "         [-256., -128.,  256.,  384.],\n",
       "         [-192., -128.,  320.,  384.],\n",
       "         [-128., -128.,  384.,  384.],\n",
       "         [ -64., -128.,  448.,  384.],\n",
       "         [   0., -128.,  512.,  384.],\n",
       "         [  64., -128.,  576.,  384.],\n",
       "         [ 128., -128.,  640.,  384.],\n",
       "         [ 192., -128.,  704.,  384.],\n",
       "         [-256.,  -64.,  256.,  448.],\n",
       "         [-192.,  -64.,  320.,  448.],\n",
       "         [-128.,  -64.,  384.,  448.],\n",
       "         [ -64.,  -64.,  448.,  448.],\n",
       "         [   0.,  -64.,  512.,  448.],\n",
       "         [  64.,  -64.,  576.,  448.],\n",
       "         [ 128.,  -64.,  640.,  448.],\n",
       "         [ 192.,  -64.,  704.,  448.],\n",
       "         [-256.,    0.,  256.,  512.],\n",
       "         [-192.,    0.,  320.,  512.],\n",
       "         [-128.,    0.,  384.,  512.],\n",
       "         [ -64.,    0.,  448.,  512.],\n",
       "         [   0.,    0.,  512.,  512.],\n",
       "         [  64.,    0.,  576.,  512.],\n",
       "         [ 128.,    0.,  640.,  512.],\n",
       "         [ 192.,    0.,  704.,  512.],\n",
       "         [-256.,   64.,  256.,  576.],\n",
       "         [-192.,   64.,  320.,  576.],\n",
       "         [-128.,   64.,  384.,  576.],\n",
       "         [ -64.,   64.,  448.,  576.],\n",
       "         [   0.,   64.,  512.,  576.],\n",
       "         [  64.,   64.,  576.,  576.],\n",
       "         [ 128.,   64.,  640.,  576.],\n",
       "         [ 192.,   64.,  704.,  576.],\n",
       "         [-256.,  128.,  256.,  640.],\n",
       "         [-192.,  128.,  320.,  640.],\n",
       "         [-128.,  128.,  384.,  640.],\n",
       "         [ -64.,  128.,  448.,  640.],\n",
       "         [   0.,  128.,  512.,  640.],\n",
       "         [  64.,  128.,  576.,  640.],\n",
       "         [ 128.,  128.,  640.,  640.],\n",
       "         [ 192.,  128.,  704.,  640.],\n",
       "         [-256.,  192.,  256.,  704.],\n",
       "         [-192.,  192.,  320.,  704.],\n",
       "         [-128.,  192.,  384.,  704.],\n",
       "         [ -64.,  192.,  448.,  704.],\n",
       "         [   0.,  192.,  512.,  704.],\n",
       "         [  64.,  192.,  576.,  704.],\n",
       "         [ 128.,  192.,  640.,  704.],\n",
       "         [ 192.,  192.,  704.,  704.]])]"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "anchor_gen.grid_anchors([(128, 128), (64, 64), (32, 32), (16, 16), (8, 8)], device='cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "b32ed8a5-3f34-47e2-b47e-c0fbadb248ac",
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-36-b4aed6fc5598>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0manchor_gen\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrid_priors\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'cpu'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/opt/conda/envs/mmdetection/lib/python3.8/site-packages/mmdet/core/anchor/anchor_generator.py\u001b[0m in \u001b[0;36mgrid_priors\u001b[0;34m(self, featmap_sizes, device)\u001b[0m\n\u001b[1;32m    229\u001b[0m                 \u001b[0mnum_base_anchors\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mnumber\u001b[0m \u001b[0mof\u001b[0m \u001b[0manchors\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mthat\u001b[0m \u001b[0mlevel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    230\u001b[0m         \"\"\"\n\u001b[0;32m--> 231\u001b[0;31m         \u001b[0;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_levels\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatmap_sizes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    232\u001b[0m         \u001b[0mmulti_level_anchors\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    233\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_levels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAssertionError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "anchor_gen.grid_priors([(2, 2), (10,10)], device='cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "f1a2a0c5-d0db-459e-b2e2-a101ec767712",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <function _TemporaryFileCloser.__del__ at 0x7fe07e412a60>\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/envs/mmdetection/lib/python3.8/tempfile.py\", line 441, in __del__\n",
      "    self.close()\n",
      "  File \"/opt/conda/envs/mmdetection/lib/python3.8/tempfile.py\", line 437, in close\n",
      "    unlink(self.name)\n",
      "FileNotFoundError: [Errno 2] No such file or directory: '/tmp/tmpfva7_j9t/tmp84yv24s7.py'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model = dict(\n",
      "    type='FasterRCNN',\n",
      "    backbone=dict(\n",
      "        type='ResNet',\n",
      "        depth=50,\n",
      "        num_stages=4,\n",
      "        out_indices=(0, 1, 2, 3),\n",
      "        frozen_stages=1,\n",
      "        norm_cfg=dict(type='BN', requires_grad=True),\n",
      "        norm_eval=True,\n",
      "        style='pytorch',\n",
      "        init_cfg=dict(type='Pretrained', checkpoint='torchvision://resnet50')),\n",
      "    neck=dict(\n",
      "        type='FPN',\n",
      "        in_channels=[256, 512, 1024, 2048],\n",
      "        out_channels=256,\n",
      "        num_outs=5),\n",
      "    rpn_head=dict(\n",
      "        type='RPNHead',\n",
      "        in_channels=256,\n",
      "        feat_channels=256,\n",
      "        anchor_generator=dict(\n",
      "            type='AnchorGenerator',\n",
      "            scales=[8],\n",
      "            ratios=[0.5, 1.0, 2.0],\n",
      "            strides=[4, 8, 16, 32, 64]),\n",
      "        bbox_coder=dict(\n",
      "            type='DeltaXYWHBBoxCoder',\n",
      "            target_means=[0.0, 0.0, 0.0, 0.0],\n",
      "            target_stds=[1.0, 1.0, 1.0, 1.0]),\n",
      "        loss_cls=dict(\n",
      "            type='CrossEntropyLoss', use_sigmoid=True, loss_weight=1.0),\n",
      "        loss_bbox=dict(type='L1Loss', loss_weight=1.0)),\n",
      "    roi_head=dict(\n",
      "        type='StandardRoIHead',\n",
      "        bbox_roi_extractor=dict(\n",
      "            type='SingleRoIExtractor',\n",
      "            roi_layer=dict(type='RoIAlign', output_size=7, sampling_ratio=0),\n",
      "            out_channels=256,\n",
      "            featmap_strides=[4, 8, 16, 32]),\n",
      "        bbox_head=dict(\n",
      "            type='Shared2FCBBoxHead',\n",
      "            in_channels=256,\n",
      "            fc_out_channels=1024,\n",
      "            roi_feat_size=7,\n",
      "            num_classes=80,\n",
      "            bbox_coder=dict(\n",
      "                type='DeltaXYWHBBoxCoder',\n",
      "                target_means=[0.0, 0.0, 0.0, 0.0],\n",
      "                target_stds=[0.1, 0.1, 0.2, 0.2]),\n",
      "            reg_class_agnostic=False,\n",
      "            loss_cls=dict(\n",
      "                type='CrossEntropyLoss', use_sigmoid=False, loss_weight=1.0),\n",
      "            loss_bbox=dict(type='L1Loss', loss_weight=1.0))),\n",
      "    train_cfg=dict(\n",
      "        rpn=dict(\n",
      "            assigner=dict(\n",
      "                type='MaxIoUAssigner',\n",
      "                pos_iou_thr=0.7,\n",
      "                neg_iou_thr=0.3,\n",
      "                min_pos_iou=0.3,\n",
      "                match_low_quality=True,\n",
      "                ignore_iof_thr=-1),\n",
      "            sampler=dict(\n",
      "                type='RandomSampler',\n",
      "                num=256,\n",
      "                pos_fraction=0.5,\n",
      "                neg_pos_ub=-1,\n",
      "                add_gt_as_proposals=False),\n",
      "            allowed_border=-1,\n",
      "            pos_weight=-1,\n",
      "            debug=False),\n",
      "        rpn_proposal=dict(\n",
      "            nms_pre=2000,\n",
      "            max_per_img=1000,\n",
      "            nms=dict(type='nms', iou_threshold=0.7),\n",
      "            min_bbox_size=0),\n",
      "        rcnn=dict(\n",
      "            assigner=dict(\n",
      "                type='MaxIoUAssigner',\n",
      "                pos_iou_thr=0.5,\n",
      "                neg_iou_thr=0.5,\n",
      "                min_pos_iou=0.5,\n",
      "                match_low_quality=False,\n",
      "                ignore_iof_thr=-1),\n",
      "            sampler=dict(\n",
      "                type='RandomSampler',\n",
      "                num=512,\n",
      "                pos_fraction=0.25,\n",
      "                neg_pos_ub=-1,\n",
      "                add_gt_as_proposals=True),\n",
      "            pos_weight=-1,\n",
      "            debug=False)),\n",
      "    test_cfg=dict(\n",
      "        rpn=dict(\n",
      "            nms_pre=1000,\n",
      "            max_per_img=1000,\n",
      "            nms=dict(type='nms', iou_threshold=0.7),\n",
      "            min_bbox_size=0),\n",
      "        rcnn=dict(\n",
      "            score_thr=0.05,\n",
      "            nms=dict(type='nms', iou_threshold=0.5),\n",
      "            max_per_img=100)))\n",
      "dataset_type = 'CocoDataset'\n",
      "data_root = '/opt/ml/detection/dataset/'\n",
      "classes = ('General trash', 'Paper', 'Paper pack', 'Metal', 'Glass', 'Plastic',\n",
      "           'Styrofoam', 'Plastic bag', 'Battery', 'Clothing')\n",
      "img_norm_cfg = dict(\n",
      "    mean=[123.675, 116.28, 103.53], std=[58.395, 57.12, 57.375], to_rgb=True)\n",
      "train_pipeline = [\n",
      "    dict(type='LoadImageFromFile'),\n",
      "    dict(type='LoadAnnotations', with_bbox=True),\n",
      "    dict(type='Resize', img_scale=(1024, 1024), keep_ratio=True),\n",
      "    dict(type='RandomFlip', flip_ratio=0.5),\n",
      "    dict(\n",
      "        type='Normalize',\n",
      "        mean=[123.675, 116.28, 103.53],\n",
      "        std=[58.395, 57.12, 57.375],\n",
      "        to_rgb=True),\n",
      "    dict(type='Pad', size_divisor=32),\n",
      "    dict(type='DefaultFormatBundle'),\n",
      "    dict(type='Collect', keys=['img', 'gt_bboxes', 'gt_labels'])\n",
      "]\n",
      "test_pipeline = [\n",
      "    dict(type='LoadImageFromFile'),\n",
      "    dict(\n",
      "        type='MultiScaleFlipAug',\n",
      "        img_scale=(1024, 1024),\n",
      "        flip=False,\n",
      "        transforms=[\n",
      "            dict(type='Resize', keep_ratio=True),\n",
      "            dict(type='RandomFlip'),\n",
      "            dict(\n",
      "                type='Normalize',\n",
      "                mean=[123.675, 116.28, 103.53],\n",
      "                std=[58.395, 57.12, 57.375],\n",
      "                to_rgb=True),\n",
      "            dict(type='Pad', size_divisor=32),\n",
      "            dict(type='ImageToTensor', keys=['img']),\n",
      "            dict(type='Collect', keys=['img'])\n",
      "        ])\n",
      "]\n",
      "data = dict(\n",
      "    samples_per_gpu=4,\n",
      "    workers_per_gpu=2,\n",
      "    train=dict(\n",
      "        type='CocoDataset',\n",
      "        classes=('General trash', 'Paper', 'Paper pack', 'Metal', 'Glass',\n",
      "                 'Plastic', 'Styrofoam', 'Plastic bag', 'Battery', 'Clothing'),\n",
      "        ann_file='/opt/ml/detection/dataset/train.json',\n",
      "        img_prefix='/opt/ml/detection/dataset/',\n",
      "        pipeline=[\n",
      "            dict(type='LoadImageFromFile'),\n",
      "            dict(type='LoadAnnotations', with_bbox=True),\n",
      "            dict(type='Resize', img_scale=(1024, 1024), keep_ratio=True),\n",
      "            dict(type='RandomFlip', flip_ratio=0.5),\n",
      "            dict(\n",
      "                type='Normalize',\n",
      "                mean=[123.675, 116.28, 103.53],\n",
      "                std=[58.395, 57.12, 57.375],\n",
      "                to_rgb=True),\n",
      "            dict(type='Pad', size_divisor=32),\n",
      "            dict(type='DefaultFormatBundle'),\n",
      "            dict(type='Collect', keys=['img', 'gt_bboxes', 'gt_labels'])\n",
      "        ]),\n",
      "    val=dict(\n",
      "        type='CocoDataset',\n",
      "        classes=('General trash', 'Paper', 'Paper pack', 'Metal', 'Glass',\n",
      "                 'Plastic', 'Styrofoam', 'Plastic bag', 'Battery', 'Clothing'),\n",
      "        ann_file='/opt/ml/detection/dataset/val_split.json',\n",
      "        img_prefix='/opt/ml/detection/dataset/',\n",
      "        pipeline=[\n",
      "            dict(type='LoadImageFromFile'),\n",
      "            dict(\n",
      "                type='MultiScaleFlipAug',\n",
      "                img_scale=(1024, 1024),\n",
      "                flip=False,\n",
      "                transforms=[\n",
      "                    dict(type='Resize', keep_ratio=True),\n",
      "                    dict(type='RandomFlip'),\n",
      "                    dict(\n",
      "                        type='Normalize',\n",
      "                        mean=[123.675, 116.28, 103.53],\n",
      "                        std=[58.395, 57.12, 57.375],\n",
      "                        to_rgb=True),\n",
      "                    dict(type='Pad', size_divisor=32),\n",
      "                    dict(type='ImageToTensor', keys=['img']),\n",
      "                    dict(type='Collect', keys=['img'])\n",
      "                ])\n",
      "        ]),\n",
      "    test=dict(\n",
      "        type='CocoDataset',\n",
      "        ann_file='/opt/ml/detection/dataset/test.json',\n",
      "        img_prefix='/opt/ml/detection/dataset/',\n",
      "        pipeline=[\n",
      "            dict(type='LoadImageFromFile'),\n",
      "            dict(\n",
      "                type='MultiScaleFlipAug',\n",
      "                img_scale=(1024, 1024),\n",
      "                flip=False,\n",
      "                transforms=[\n",
      "                    dict(type='Resize', keep_ratio=True),\n",
      "                    dict(type='RandomFlip'),\n",
      "                    dict(\n",
      "                        type='Normalize',\n",
      "                        mean=[123.675, 116.28, 103.53],\n",
      "                        std=[58.395, 57.12, 57.375],\n",
      "                        to_rgb=True),\n",
      "                    dict(type='Pad', size_divisor=32),\n",
      "                    dict(type='ImageToTensor', keys=['img']),\n",
      "                    dict(type='Collect', keys=['img'])\n",
      "                ])\n",
      "        ]))\n",
      "evaluation = dict(interval=1, metric='bbox')\n",
      "optimizer = dict(type='SGD', lr=0.02, momentum=0.9, weight_decay=0.0001)\n",
      "optimizer_config = dict(grad_clip=None)\n",
      "lr_config = dict(\n",
      "    policy='step',\n",
      "    warmup='linear',\n",
      "    warmup_iters=500,\n",
      "    warmup_ratio=0.001,\n",
      "    step=[16, 22])\n",
      "runner = dict(type='EpochBasedRunner', max_epochs=24)\n",
      "checkpoint_config = dict(interval=1)\n",
      "log_config = dict(interval=50, hooks=[dict(type='TextLoggerHook')])\n",
      "custom_hooks = [dict(type='NumClassCheckHook')]\n",
      "dist_params = dict(backend='nccl')\n",
      "log_level = 'INFO'\n",
      "load_from = None\n",
      "resume_from = None\n",
      "workflow = [('train', 1)]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cfg = Config.fromfile('/opt/ml/detection/mmdetection/my_configs/faster_rcnn_base.py')\n",
    "print(cfg.pretty_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dae64289-1999-4a39-8b82-c3e1814a77e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "        >>> all_anchors = self.grid_anchors([(2, 2)], device='cpu')\n",
    "        >>> print(all_anchors)\n",
    "        [tensor([[-4.5000, -4.5000,  4.5000,  4.5000],\n",
    "                [11.5000, -4.5000, 20.5000,  4.5000],\n",
    "                [-4.5000, 11.5000,  4.5000, 20.5000],\n",
    "                [11.5000, 11.5000, 20.5000, 20.5000]])]\n",
    "        >>> self = AnchorGenerator([16, 32], [1.], [1.], [9, 18])\n",
    "        >>> all_anchors = self.grid_anchors([(2, 2), (1, 1)], device='cpu')\n",
    "        >>> print(all_anchors)\n",
    "        [tensor([[-4.5000, -4.5000,  4.5000,  4.5000],\n",
    "                [11.5000, -4.5000, 20.5000,  4.5000],\n",
    "                [-4.5000, 11.5000,  4.5000, 20.5000],\n",
    "                [11.5000, 11.5000, 20.5000, 20.5000]]), \\\n",
    "        tensor([[-9., -9., 9., 9.]])]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mmdetectron",
   "language": "python",
   "name": "mmdetectron"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
